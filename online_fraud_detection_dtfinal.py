# -*- coding: utf-8 -*-
"""online_fraud_detection_dtfinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18cnTp-BmTQPssBQnf6fQe78VOUcVGfLa
"""

!pip install kaggle

from google.colab import files
files.upload()

! mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets list

!kaggle datasets download -d jainilcoder/online-payment-fraud-detection

!unzip online-payment-fraud-detection.zip

import pandas as pd
import numpy as np

data= pd.read_csv("/content/online-payment-fraud-detection.zip")
data.head(10)

data.describe()

data.info()

duplicates_info = data[data.duplicated()]
print(duplicates_info)



print(data.isnull().sum())

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize the distribution of 'isFraud' and 'isFlaggedFraud'
sns.countplot(x='isFraud', data=data)
plt.title('Distribution of isFraud')
plt.show()

sns.countplot(x='isFlaggedFraud', data=data)
plt.title('Distribution of isFlaggedFraud')
plt.show()

data.type.value_counts()

type_counts = data['type'].value_counts()

# Accessing the transaction types and their quantities
transaction = type_counts.index
quantity = type_counts.values

import plotly.express as px
figure = px.pie(data, values = quantity, names = transaction, hole = 0.5, title = "Distribution of Transaction type")
figure.show()

data.type.value_counts()

type = data['type'].value_counts()
type

corr = data.corr()
corr

data['type'] = data['type'].map({'CASH_OUT':1, 'PAYMENT':2, 'CASH_IN':3, 'TRANSFER':4, 'DEBIT':5})

# data['isFraud'] = data['isFraud'].map({0:'No_Fraud', 1:'Fraud'})

data.head(5)

from sklearn.model_selection import train_test_split

X = np.array(data[['type', 'amount', 'oldbalanceOrg', 'newbalanceOrig','oldbalanceDest','newbalanceDest']])

y = np.array(data[['isFraud']])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)

from sklearn.tree import DecisionTreeClassifier

model_dt = DecisionTreeClassifier()

model_dt.fit(X_train, y_train)

model_dt.score(X_test, y_test)

data = np.array([[1,121558,0,0,1248672,1370230]])
model_dt.predict(data)

data1 = np.array([[4,88424,88424,0,0,0]])
model_dt.predict(data1)

from imblearn.under_sampling import RandomUnderSampler

# Assuming X and y are your features and target variable respectively
# Perform majority undersampling to address the imbalanced classes in the dataset
undersample = RandomUnderSampler(sampling_strategy='majority')

# Obtaining the undersampled dataframes - testing and training
X_resampled, y_resampled = undersample.fit_resample(X, y)

print(y_resampled.shape)

data = np.array([[1,121558,0,0,1248672,1370230]])
model_dt.predict(data)

data1 = np.array([[4,88424,88424,0,0,0]])
model_dt.predict(data1)

model_dt.score(X_test, y_test)

from sklearn.metrics import precision_score, recall_score, f1_score

from mmap import MAP_DENYWRITE

# Instantiate the logistic regression model
md = DecisionTreeClassifier()

# Train the model
md.fit(X_train, y_train)

# Make predictions on the test set
y_pred = md.predict(X_test)

precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

print(y_resampled.shape)

import pandas as pd

# Assuming y_resampled is your target variable after undersampling
# Convert y_resampled to a DataFrame for easier analysis
y_resampled_df = pd.DataFrame(y_resampled, columns=['isFraud'])

# Check the distribution of classes in the undersampled dataset
class_distribution = y_resampled_df['isFraud'].value_counts()
print("Class distribution after undersampling:")
print(class_distribution)

# Calculate the ratio of minority class to majority class
minority_class_count = class_distribution[1]  # Assuming 1 represents the minority class
majority_class_count = class_distribution[0]  # Assuming 0 represents the majority class
balance_ratio = minority_class_count / majority_class_count

# Check if the dataset is balanced based on some threshold (e.g., balance_ratio close to 1)
if balance_ratio >= 0.5 and balance_ratio <= 2:
    print("The dataset is balanced.")
else:
    print("The dataset is not balanced.")

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

from xgboost import XGBClassifier
from xgboost import plot_importance

# Commented out IPython magic to ensure Python compatibility.
# %%time
# xgb = XGBClassifier(objective='binary:logistic', random_state=0)
# 
# cv_params = {'max_depth': [4,5,6,7,8],
#              'min_child_weight': [1,2,3,4,5],
#              'learning_rate': [0.05,0.1, 0.2, 0.3],
#              'n_estimators': [75, 100, 125]  # number of trees
#              }
# 
# scoring = {'accuracy', 'precision', 'recall', 'f1'}
# 
# xgb_cv = RandomizedSearchCV(xgb, cv_params, scoring=scoring, cv=5, refit='f1',n_iter=50)
# 
# 
# xgb_cv.fit(X_train, y_train)

import pickle
with open('model.pkl','wb') as file:
  pickle.dump(model_dt,file)